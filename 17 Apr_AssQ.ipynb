{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76b5add",
   "metadata": {},
   "source": [
    "#### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9298a",
   "metadata": {},
   "source": [
    "ANs: Gradient Boosting Regression is a machine learning algorithm used for regression tasks. It combines multiple weak regression models to create a strong predictive model. It iteratively builds models that focus on correcting the mistakes made by the previous models, minimizing the difference between the predicted values and the actual target values. The final prediction is the sum of the predictions made by all the models in the ensemble. Gradient Boosting Regression is effective in handling complex relationships and providing accurate regression predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991775b",
   "metadata": {},
   "source": [
    "\n",
    "#### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use asimple regression problem as an example and train the model on a small dataset. Evaluate the model's  performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbaabbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9.000000001411015\n",
      "R-squared: -3.5000000007055077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the prediction with the mean of y\n",
    "        prediction = np.mean(y) * np.ones(len(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the negative gradient (residuals)\n",
    "            residuals = y - prediction\n",
    "            \n",
    "            # Fit a decision tree on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the prediction\n",
    "            update = self.learning_rate * tree.predict(X)\n",
    "            prediction += update\n",
    "            \n",
    "            # Store the weak model\n",
    "            self.models.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "         # Initialize the predictions\n",
    "        predictions = np.zeros(len(X))\n",
    "        \n",
    "        # Make predictions using all weak models\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Create a simple dataset\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([1, 3, 2, 4, 5])\n",
    "\n",
    "# Create and train the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_pred = regressor.predict(X)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "r2 = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d04c33c",
   "metadata": {},
   "source": [
    "#### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faca3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a sample regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a sample regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2234be",
   "metadata": {},
   "source": [
    "#### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d24540",
   "metadata": {},
   "source": [
    "Ans: In the context of Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that is used as a building block within the ensemble of models. Weak learners are often decision trees with shallow depth or limited number of splits.\n",
    "\n",
    "The idea behind using weak learners in Gradient Boosting is that although each individual weak learner may have limited predictive power, the ensemble of weak learners can combine their strengths and collectively build a more powerful and accurate predictive model. Each weak learner is trained to focus on the patterns or residuals that were not captured well by the previous models in the ensemble.\n",
    "\n",
    "By sequentially adding weak learners to the ensemble and updating the predictions based on the errors made by the previous models, Gradient Boosting can iteratively reduce the errors and improve the overall performance. The weak learners are designed to complement each other and contribute to the ensemble by targeting the specific areas of the data where the previous models have performed poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a450a2",
   "metadata": {},
   "source": [
    "#### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470b24e",
   "metadata": {},
   "source": [
    "ANs: The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "- **Starting point**: We begin with an initial prediction, which is typically a simple estimate such as the average of the target values in the training dataset.\n",
    "\n",
    "- **Iterative learning**: We sequentially build a series of weak models, also known as weak learners or base models, such as decision trees. Each weak model is trained to predict the residuals or errors of the previous models.\n",
    "\n",
    "- **Correcting mistakes**: In each iteration, the weak model is focused on learning the patterns or relationships that the previous models failed to capture. It aims to minimize the difference between the actual target values and the predictions made by the ensemble of models.\n",
    "\n",
    "- **Ensemble of models**: The predictions from all the weak models are combined by adding them together. This ensemble of models collectively creates a more accurate and powerful predictive model.\n",
    "\n",
    "- **Learning rate**: Each weak model's contribution to the final prediction is controlled by a learning rate parameter. It determines the step size at which the model updates the predictions.\n",
    "\n",
    "- **Iterative improvement**: The process continues iteratively, with each new weak model adjusting the predictions based on the errors made by the previous models. The goal is to gradually reduce the errors and improve the overall prediction accuracy.\n",
    "\n",
    "- **Final prediction**: The final prediction is obtained by summing the predictions made by all the weak models in the ensemble.\n",
    "\n",
    "The intuition behind Gradient Boosting is that by iteratively building and combining weak models, each focusing on correcting the mistakes of the previous models, we can create a powerful ensemble model that captures complex relationships and achieves high prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc7347",
   "metadata": {},
   "source": [
    "#### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c560ac",
   "metadata": {},
   "source": [
    "Ans: The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. Here's a step-by-step explanation of how the algorithm constructs the ensemble:\n",
    "\n",
    "- **Initialization**: The algorithm starts with an initial prediction, which is often a simple estimate like the average of the target values in the training dataset.\n",
    "\n",
    "- **Calculate Residuals**: The algorithm calculates the differences between the actual target values and the current predictions. These differences, known as residuals or errors, represent the areas where the current model performs poorly.\n",
    "\n",
    "- **Train a Weak Learner**: A weak learner, typically a decision tree with a small depth, is trained to predict the residuals. The goal is to find a model that can capture the patterns or relationships missed by the current ensemble.\n",
    "\n",
    "- **Update Predictions**: The predictions of the weak learner are scaled by a learning rate and added to the current ensemble's predictions. This step adjusts the predictions, moving them closer to the true values and reducing the overall error.\n",
    "\n",
    "- **Repeat Steps 2-4**: The process is repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, new weak learners are trained on the residuals, and their predictions are added to the ensemble.\n",
    "\n",
    "- **Final Prediction**: The final prediction is obtained by summing the predictions made by all the weak learners in the ensemble. This aggregated prediction represents the combined knowledge of all the weak learners and typically provides a more accurate result than any individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b634681",
   "metadata": {},
   "source": [
    "#### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4b762",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves several steps. Here's an overview of the key steps involved:\n",
    "\n",
    "- **Define the Loss Function**: The first step is to define a loss function that measures the difference between the predicted values and the actual target values. Common loss functions for regression problems include mean squared error (MSE) and mean absolute error (MAE).\n",
    "\n",
    "- **Initialize the Model**: The algorithm starts by initializing the model with an initial prediction, such as the average of the target values. This initial prediction serves as the starting point for building the ensemble.\n",
    "\n",
    "- **Calculate the Negative Gradient**: The negative gradient of the loss function with respect to the current predictions is computed. This gradient represents the direction and magnitude of the steepest descent, indicating how the predictions should be adjusted to minimize the loss.\n",
    "\n",
    "- **Train a Weak Learner** : A weak learner, often a decision tree with limited depth, is trained to predict the negative gradient. The weak learner is fitted to the training data, using the current predictions as the target variable and the negative gradient as the input feature.\n",
    "\n",
    "- **Update the Predictions**: The predictions of the weak learner are scaled by a learning rate (a small fraction) and added to the current ensemble's predictions. This update step adjusts the predictions, moving them towards the optimal values and reducing the loss.\n",
    "\n",
    "- **Repeat Steps 3-5**: The process is repeated iteratively for a specified number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained on the negative gradients, and its predictions are added to the ensemble.\n",
    "\n",
    "- **Final Prediction**: The final prediction is obtained by summing the predictions made by all the weak learners in the ensemble. This aggregated prediction represents the combined knowledge of all the weak learners and provides the final output of the Gradient Boosting algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
