{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a2344f",
   "metadata": {},
   "source": [
    "#### Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cd524",
   "metadata": {},
   "source": [
    "Ans: Ridge Regression is a regularization technique to handle overfitting and collinearity. It is a L2 norm regularization. When a model has high variance and low bias it means that our model has captured too much noise and is unable to fit a line to new incoming data. It looses its ability to generalize on unseen dataset. This is known as overfitting. When our model has high variance that means it has multi-collinearity in dataset. This often lead to overfitting, to tackle such case we make use of Ridge regression. \n",
    "\n",
    "Ridge regression adds penalty factor to loss_function to induce a small amount of bias which will decrease variance and hence overfitting. It also tends to reduce model complexity and reduce amount of training dataset (noise) to reduce overfitting.\n",
    "\n",
    "Equation of ridge regression:\n",
    "\n",
    "loss_function = Residual Sum of Squares + λ * (Sum of the squared value of the coefficients)\n",
    "\n",
    "Where SSR = Residual sum of squares\n",
    "\n",
    "λ = tuning parameter for coefficienet (slope)\n",
    "\n",
    "When we increase the value of lambda the value of slope coefficient decreases and tends to reach zero and when this factor is added to residual sum of squares we get a best fit line with low variance and low bias which reduces the effect of overfitting. But we should be careful with value of lambda beacuse we have to establish a bias-variance tradeoff to lower variance of overall model otherwise we may induce too much bias which can lead to underfitting.\n",
    "\n",
    "\n",
    "- Ridge regression compensate for overfitting by inducing penalty_factor/ridge_estimator in loss_function whereas in ordinary least squares regression is simply loss_function with difference between the actual value and predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b62c7",
   "metadata": {},
   "source": [
    "#### Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c734e3",
   "metadata": {},
   "source": [
    "The assumptions are same as linear model i.e linear regression i.e\n",
    "\n",
    "a) linear relationship\n",
    "\n",
    "b) contant variance \n",
    "\n",
    "c) independence in observations, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c5a96",
   "metadata": {},
   "source": [
    "#### Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d5395",
   "metadata": {},
   "source": [
    "The regularization parameter is selected by using cross-validation specifically 10 fold cross-validation. This is done by splitting the dataset into training, validation and testing. we train the model on training set and then use different settings for parameter values with validation set to check the optimal performance of our model. The particular model with optimal value of parameter will be choosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2387a0",
   "metadata": {},
   "source": [
    "#### Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc7c08",
   "metadata": {},
   "source": [
    "Ans: No Ridge Regression cannot be used for feature selection as it dosen't shrink coefficients to zero instead it leads towards zero but never equal to zero. Because of this it cannot perform feature selection instead we use lasso or elastic regression for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9561d0",
   "metadata": {},
   "source": [
    "#### Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b79c54",
   "metadata": {},
   "source": [
    "Ridge regression coefficients in loss_fucntions shrinks towards zero but not zero, this reduces both multlicollinearity and overfitting as the coefficinet will not change drastically as change in input. This will reduce dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475d331",
   "metadata": {},
   "source": [
    "#### Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b26eb",
   "metadata": {},
   "source": [
    "No, Ridge Regression cannot directly use categorical variables but can use continuous variables. For categorical variables they need to be encoded into continuous variables using techniques such as oridnal, one hot encoding, target guided encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c2564",
   "metadata": {},
   "source": [
    "#### Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1360e3",
   "metadata": {},
   "source": [
    "Coefficients denotes the change in the output i.e target variables and it also denotes the direction of change through signs. In ridge regression model we have a penalty factor in loss_function. This penalty factor penalizes high coefficients by shrinking them toward zero but not equal to zero. This is done through a peanlty factor called lambda. If the value for lambda is set too high, the coefficients moves towards zero. Those coefficients whose magnitude decreases does not change drastically upon change in input this reduces the effect of overfitting and multicollinearity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aae683",
   "metadata": {},
   "source": [
    "#### Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec65b25",
   "metadata": {},
   "source": [
    "Yes, we can use ridge regression for time-series data analysis. Examples include stock market prediction, commodity price prediction,etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
